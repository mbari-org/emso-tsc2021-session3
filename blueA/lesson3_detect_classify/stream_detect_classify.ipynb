{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ![alt text](https://www.mbari.org/wp-content/uploads/2014/11/logo-mbari-3b.png \"MBARI\")\n",
    "\n",
    "  <div align=\"left\">Copyright (c) 2021, MBARI</div>\n",
    "\n",
    "  * Distributed under the terms of the GPL License\n",
    "  * Maintainer: dcline@mbari.org\n",
    "  * Authors: Danelle Cline dcline@mbari.org, John Ryan ryjo@mbari.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blue A Stream Detection\n",
    "\n",
    "This notebook demonstrates how to predict blue A calls on a spectrogram incrementally, leveraging PCEN streaming.  This is useful when an audio file is large,  or if streaming data directly from a hydrophone. This is an alternative to using BLEDs for detection. \n",
    "\n",
    "\n",
    "## Install dependencies\n",
    "\n",
    "First, let's install dependencies and include all packages used in this tutorial. This only needs to be done once for the duration of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install oceansoundscape==1.1.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "import numpy as np\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import soundfile as sf\n",
    "import scipy\n",
    "import sklearn\n",
    "import shutil\n",
    "import librosa as librosa\n",
    "import librosa.display as display \n",
    "from librosa.display import specshow, waveplot\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import requests\n",
    "import timeit\n",
    "import json\n",
    "from numpy.lib import stride_tricks\n",
    "import io\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from oceansoundscape.spectrogram import conf, colormap\n",
    "from oceansoundscape.spectrogram.utils import ImageUtils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, download an audio file to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's download the data used in this notebook\n",
    "bucket = 'emso-tsc2021-session3-eu-west-3'\n",
    "# wav_filename = 'blue_A_stream.wav' # shorter 5 minute examples\n",
    "wav_filename = 'MARS-20171101T000000Z-10min-2kHz.wav' \n",
    "#'MARS-20171101T000000Z-2kHz.wav' # full-day example\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "    aws_access_key_id='',\n",
    "    aws_secret_access_key='',\n",
    "    config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "# only download if needed\n",
    "if not Path(wav_filename).exists():\n",
    "    print('Downloading')\n",
    "    s3.Bucket(bucket).download_file(wav_filename, wav_filename)\n",
    "    print(f'Done downloading {wav_filename}')\n",
    "\n",
    "samples, sample_rate = sf.read(wav_filename,dtype='float32')\n",
    "nsec = (samples.size)/sample_rate # number of seconds in vector\n",
    "print(f'Read {nsec} seconds of data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCEN Streaming\n",
    "\n",
    "Here, can use the streaming IO with librosa.pcen to do dynamic per-channel energy normalization (PCEN) on a spectrogram incrementally. \n",
    "\n",
    "First, set up the block reader to work on audio segments at least the length of an expected call, overlapping prediction by 75% with adjacent frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimum configuration for the call spectrogram generation\n",
    "# is defined in the oceanscoundscape package based on extensive\n",
    "# hyper parameter sweeps. These need to match those used to train the model\n",
    "blue_a_conf = conf.CONF_DICT['blueA']\n",
    "\n",
    "# this is a global for all call types\n",
    "fft_overlap = conf.OVERLAP \n",
    "\n",
    "# fft window and overlap - should be the same used in training the model\n",
    "num_fft = blue_a_conf['num_fft']\n",
    "hop_length = int(num_fft * (1 - fft_overlap)) \n",
    "call_duration_secs = blue_a_conf['duration_secs'] \n",
    "secs_per_frame = hop_length / sample_rate\n",
    "\n",
    "# the axis to blur during spectrogram generation; freq or time or empty for no blurring\n",
    "# should be the same as used in training\n",
    "blur_axis = blue_a_conf['blur_axis'] \n",
    "\n",
    "# Block 20x the length of a call window\n",
    "block_length = int(20*call_duration_secs/secs_per_frame)\n",
    " \n",
    "# Overlap window for prediction by 75% \n",
    "pred_overlap = .75\n",
    "overlap = int(call_duration_secs*pred_overlap/secs_per_frame)\n",
    " \n",
    "window_size = call_duration_secs/secs_per_frame\n",
    "step_size = window_size - overlap\n",
    "num_segments = int(block_length/step_size)\n",
    "\n",
    "freq_min = blue_a_conf['low_freq']\n",
    "freq_max =  blue_a_conf['high_freq']\n",
    "\n",
    "# PCEN parameters\n",
    "pcen_gain = conf.PCEN_GAIN\n",
    "pcen_bias = conf.PCEN_BIAS\n",
    "pcen_tc = conf.PCEN_TIME_CONSTANT\n",
    "num_mels = blue_a_conf['num_mels']\n",
    "\n",
    "def stream_init():\n",
    "    \"\"\"\n",
    "    Utility function to reinialize the stream\n",
    "    \"\"\"\n",
    "    return librosa.stream(wav_filename, block_length=block_length,\n",
    "                            frame_length=num_fft,\n",
    "                            hop_length=hop_length,\n",
    "                            mono=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Striding\n",
    "\n",
    "After computing the spectrogram one could simply run a non-overlapping window across the spectrogram, but that could miss a call if it landed on the boundary of a window.  To remedy this, striding the spectrogram with overlapping segments is needed which is simplified with the method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_views(arr, win_size, step_size, writeable = False):\n",
    "  \"\"\"\n",
    "  # Credit to Kevin Urban's blog for the code below to simplify striding\n",
    "  https://krbnite.github.io/Memory-Efficient-Windowing-of-Time-Series-Data-in-Python-3-Memory-Strides-in-Pandas/\n",
    "  arr: any 2D array whose columns are distinct variables and \n",
    "    rows are data records at some timestamp t\n",
    "  win_size: size of data window (given in data points along record/time axis)\n",
    "  step_size: size of window step (given in data point along record/time axis)\n",
    "  writable: if True, elements can be modified in new data structure, which will affect\n",
    "    original array (defaults to False)\n",
    "  \n",
    "  Note that step_size is related to window overlap (overlap = win_size - step_size), in \n",
    "  case you think in overlaps.\n",
    "  \"\"\"\n",
    "  \n",
    "  # If DataFrame, use only underlying NumPy array\n",
    "  if type(arr) == type(pd.DataFrame()):\n",
    "    arr = arr.values\n",
    "  \n",
    "  # Compute Shape Parameter for as_strided\n",
    "  n_records = arr.shape[0]\n",
    "  n_columns = arr.shape[1]\n",
    "  remainder = (n_records - win_size) % step_size \n",
    "  # Note  - bug fix here - add 2 not 1 as in the blog\n",
    "  num_windows = 2 + int((n_records - win_size - remainder) / step_size) \n",
    "  shape = (num_windows, win_size, n_columns)\n",
    "  \n",
    "  # Compute Strides Parameter for as_strided\n",
    "  next_win = step_size * arr.strides[0]\n",
    "  next_row, next_col = arr.strides\n",
    "  strides = (next_win, next_row, next_col)\n",
    "    \n",
    "  print(f'shape {shape} strides {strides}')\n",
    "\n",
    "  new_view_structure = stride_tricks.as_strided(\n",
    "    arr,\n",
    "    shape = shape,\n",
    "    strides = strides,\n",
    "    writeable = writeable,\n",
    "  )\n",
    "  return new_view_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Visualize overlapping spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the PCEN filter delays to steady state\n",
    "zi = None\n",
    "stream = stream_init()\n",
    "\n",
    "# Initialize figure with subplots to visualize the overlap in the first block\n",
    "fig2, axes = plt.subplots(21, 1)\n",
    "fig2.set_size_inches(6, 12)\n",
    "\n",
    "for y_block in stream:\n",
    "    \n",
    "    D = librosa.feature.melspectrogram(sklearn.preprocessing.minmax_scale(y_block, feature_range=((-2 ** 31), (2 ** 31))), \n",
    "            sr=sample_rate, center=True, hop_length=hop_length, power=1, \n",
    "                                       n_mels=num_mels, fmin=freq_min, fmax=freq_max)\n",
    "\n",
    "    # Compute PCEN on the mel spectrum using initial delays (zi)\n",
    "    P, zi =  librosa.pcen((2**31)*D, sr=sample_rate, hop_length=hop_length, gain=pcen_gain, bias=pcen_bias, \n",
    "                          time_constant=pcen_tc, zi=zi, return_zf=True)\n",
    "    \n",
    "    # Create strided view\n",
    "    strided = make_views(P.transpose(), win_size=int(window_size), step_size=int(step_size))\n",
    "     \n",
    "    librosa.display.specshow(P, sr=sample_rate, fmin=freq_min, fmax=freq_max, cmap=colormap.parula_map,\n",
    "                         hop_length=hop_length, x_axis='time', y_axis='mel', ax=axes[0])\n",
    "    \n",
    "    # Display the first 20 overlapping segments\n",
    "    for i, s in enumerate(strided):\n",
    "        if i > 19:\n",
    "            break\n",
    "        librosa.display.specshow(utils.smooth(s.transpose(), blur_axis), sr=sample_rate, fmin=freq_min, fmax=freq_max, \n",
    "                                 cmap=colormap.parula_map, hop_length=hop_length, x_axis='time', \n",
    "                                 y_axis='mel', ax=axes[i+1]) \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'pacific-sound-models'\n",
    "model_filename = 'bluewhale-a-resnet50-2021-09-22-21-05-23-858.tar.gz' \n",
    "\n",
    "# only download if needed\n",
    "if not Path(model_filename).exists(): \n",
    "    print(f'Downloading...') \n",
    "    s3.Bucket(bucket).download_file(key, model_filename)\n",
    "\n",
    "# Alternatively, it can be downloaded directly in SageMaker with\n",
    "# !aws s3 cp s3://{bucket}/{key} . \n",
    " \n",
    "print(f'Uncompressing')\n",
    "!tar -xf {model_filename}\n",
    "print(f'Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open('1/config.json'))\n",
    "image_mean = np.asarray(config[\"image_mean\"])\n",
    "image_std = np.asarray(config[\"image_std\"])\n",
    "print(f\"Labels {config['classes']}\")\n",
    "print(f\"Training image mean: {image_mean}\")\n",
    "print(f\"Training image std: {image_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the classifier over each optimized segment\n",
    "\n",
    "Here, we take the pcen computed segment, further preprocess it in the same manner the training data was preprocessed. This preprocessing applies the same colormap, smooths the image in the frequency domain, then denoises the colorized spectrogram in the color domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zi = None\n",
    "stream = stream_init()\n",
    "batch_size = 1\n",
    "secs = 0\n",
    "\n",
    "df = pd.DataFrame(columns=[\"time_secs\", \"score_baf\", \"score_bat\"]) \n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "for y_block in stream:\n",
    "                          \n",
    "    D = librosa.feature.melspectrogram(sklearn.preprocessing.minmax_scale(y_block, feature_range=(-2 ** 31, 2 ** 31)), \n",
    "                                       sr = sample_rate, \n",
    "                                       center = False, \n",
    "                                       hop_length = hop_length, \n",
    "                                       power = 1, \n",
    "                                       n_mels = num_mels, \n",
    "                                       fmin = freq_min, \n",
    "                                       fmax = freq_max)\n",
    " \n",
    "    # Compute PCEN on the mel spectrum using initial delays (zi)\n",
    "    P, zi =  librosa.pcen((2**31)*D, \n",
    "                          sr = sample_rate, \n",
    "                          hop_length = hop_length, \n",
    "                          gain = pcen_gain, \n",
    "                          bias = pcen_bias, \n",
    "                          time_constant = pcen_tc, \n",
    "                          zi = zi, \n",
    "                          return_zf = True)\n",
    "          \n",
    "    # Create strided view\n",
    "    strided = make_views(P.transpose(), win_size=int(window_size), step_size=int(step_size)) \n",
    "    \n",
    "    for i, s in enumerate(strided): \n",
    "        image_path = Path(f'block{y_block[0]}_stride{i}.jpg')\n",
    "        \n",
    "        strided_smoothed = utils.smooth(s.transpose(), blur_axis)\n",
    "        strided_colored = utils.colorizeDenoise(strided_smoothed, image_path)\n",
    "        image_bgr = cv2.imread(image_path.as_posix())\n",
    "        image = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # normalize with the same parameters used in training\n",
    "        image_float = np.asarray(image).astype('float32')\n",
    "        image_float = image_float / 255.0\n",
    "        image_float = (image_float - image_mean) / image_std\n",
    "\n",
    "        image = np.concatenate([image_float[np.newaxis, :, :]] * batch_size)\n",
    "        tensor_out = model(image)\n",
    "        score_baf, score_bat = tensor_out.numpy()[0]\n",
    "        \n",
    "        print(f'Processing segment {i} bat {score_bat} baf {score_baf} start_time {secs}')\n",
    "        secs += step_size*secs_per_frame\n",
    "            \n",
    "        df = df.append({'time_secs': secs, 'score_baf': score_baf, 'score_bat': score_bat}, ignore_index=True)\n",
    "        \n",
    "        # uncomment the following line to save the classification image along with an encoded score in the filename\n",
    "        # this can be useful for visual browsing\n",
    "        # shutil.copy2(image_path, f'block{y_block[0]}_stride{i}_{int(score_bat*100):02}.jpg')\n",
    "        \n",
    "        # remove the image \n",
    "        image_path.unlink()\n",
    "\n",
    "total_seconds = timeit.default_timer() - start_time \n",
    "print(f'Done. Processed {nsec} seconds of data in {total_seconds} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot model scores\n",
    "\n",
    "Here, we plot the true scores which could be further processed with, e.g. a median filter, then a suitable threshold could be chosen to compute a time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(32, 8))\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[1, 1])\n",
    " \n",
    "D = librosa.feature.melspectrogram(sklearn.preprocessing.minmax_scale(samples, feature_range=(-2 ** 31, 2 ** 31)), \n",
    "                                   sr = sample_rate, \n",
    "                                   center = False, \n",
    "                                   hop_length = hop_length, \n",
    "                                   power = 1, \n",
    "                                   n_mels = num_mels, \n",
    "                                   fmin = freq_min, \n",
    "                                   fmax = freq_max)\n",
    " \n",
    "P =  librosa.pcen((2**31)*D, \n",
    "                  sr = sample_rate, \n",
    "                  hop_length = hop_length, \n",
    "                  gain = pcen_gain, \n",
    "                  bias = pcen_bias, \n",
    "                  time_constant = pcen_tc)\n",
    " \n",
    "plt.subplot(gs[0])\n",
    "librosa.display.specshow(utils.smooth(P, blur_axis), \n",
    "                         sr = sample_rate, \n",
    "                         fmin = freq_min, \n",
    "                         fmax = freq_max, \n",
    "                         cmap = colormap.parula_map, \n",
    "                         hop_length = hop_length, \n",
    "                         x_axis='time', y_axis='mel')\n",
    " \n",
    "plt.subplot(gs[1])\n",
    "plt.plot(df.time_secs, df.score_bat, 'o', color='black', markersize=9)\n",
    "plt.ylabel('Model Blue A True Score')\n",
    "plt.xlabel('Seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emso-tsc2021-session3",
   "language": "python",
   "name": "emso-tsc2021-session3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
